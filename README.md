# Decision-Tree


Introduction Decision Trees are a type of Supervised Machine Learning (that is you explain what the input is and what the corresponding output is in the training data) where the data is continuously split according to a certain parameter.

Applications of Decision Trees
1. Assessing prospective growth opportunities
One of the applications of decision trees involves evaluating prospective growth opportunities for businesses based on historical data. Historical data on sales can be used in decision trees that may lead to making radical changes in the strategy of a business to help aid expansion and growth.

2. Using demographic data to find prospective clients
Another application of decision trees is in the use of demographic data to find prospective clients. They can help streamline a marketing budget and make informed decisions on the target market that the business is focused on. In the absence of decision trees, the business may spend its marketing market without a specific demographic in mind, which will affect its overall revenues.

3. Serving as a support tool in several fields
Lenders also use decision trees to predict the probability of a customer defaulting on a loan by applying predictive model generation using the client’s past data. The use of a decision tree support tool can help lenders evaluate a customer’s creditworthiness to prevent losses.

Decision trees can also be used in operations research in planning logistics and strategic management. They can help in determining appropriate strategies that will help a company achieve its intended goals. Other fields where decision trees can be applied include engineering, education, law, business, healthcare, and finance.


Implementation  of a decision tree induction algorithm for classification tasks and to demonstrate that it works as expected. The algorithm shall process data sets according to an approved standard 
Chooses whether to use information entropy or gini impurity as split criterion. 
To calculate binary splits for real-valued features, the following rule must be applied:
an instance with a feature value lower than the mean feature value follows the left edge from the split node while all other instances follow the right edge from the split node. 
Demonstrate that the algorithm works as expected on three classification data sets: 
Iris3, Wine4, and one additional data set of your own choice from the UCI machine learning repository. 
In order to run the project without any error please follow the following instructions:
1. Search Anacoda Prompt and click it to run.
2. run following command
conda install pydot
enter "y" if asked
3. open provided .ipynb file and run cell-by-cell
